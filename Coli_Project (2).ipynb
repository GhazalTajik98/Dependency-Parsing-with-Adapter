{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4C2jThULtfH"
      },
      "source": [
        "### Data Loading and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q0QyYzJR1cc",
        "outputId": "def22a22-8d58-460e-852d-b201ba009b5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.6.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ndgmkXvODw7",
        "outputId": "c6cf141b-60ce-475c-9379-9d7a3e17f487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 123ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 101ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 109ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install requirement libraries, packages\n",
        "!uv pip install datasets\n",
        "!uv pip install conllu\n",
        "!uv pip install torchviz\n",
        "\n",
        "!uv pip install wandb\n",
        "!uv pip install ufal.chu-liu-edmonds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AfN8PXKHLpVy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUIJIySByY13",
        "outputId": "915af7d7-b54c-42b1-c3c9-8de1098bb0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from config import DATASET_PATH, DATASET_NAME, EXPERIMENT_NAME, RELATION_NUM, HIDDEN_DIM, OUTPUT_DIM\n",
        "from data import dataset_reading_and_encoding, print_first_batch\n",
        "from models import model_initializing\n",
        "from utils import count_parameters\n",
        "from train import train\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process datasets\n",
        "dataset = load_dataset(path=DATASET_PATH, name=DATASET_NAME, trust_remote_code=True)\n",
        "data = dataset_reading_and_encoding(dataset)\n",
        "print_first_batch(data[\"train\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yBiC0D9-yYB",
        "outputId": "1704075e-f7de-4359-d867-e5bd0a80f3b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Batch:\n",
            "input_ids shape: torch.Size([32, 200])\n",
            "attention_mask shape: torch.Size([32, 200])\n",
            "head shape: torch.Size([32, 200])\n",
            "deprel_ids shape: torch.Size([32, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train base model\n",
        "base_model = model_initializing(\"base\", hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM, relation_num=RELATION_NUM)\n",
        "count_parameters(base_model)\n",
        "base_model = train(base_model, data, EXPERIMENT_NAME, save_model=True, model_name=\"base_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "FRpuB4vg-4U7",
        "outputId": "83718517-0cda-4c72-86cb-05404512e2f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 279,346,968\n",
            "Trainable parameters: 29,654,808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mghta00001\u001b[0m (\u001b[33mghta00001-university-of-saarland\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250323_124721-1bq4ylg3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ghta00001-university-of-saarland/TokenDependency/runs/1bq4ylg3' target=\"_blank\">DataDragon2</a></strong> to <a href='https://wandb.ai/ghta00001-university-of-saarland/TokenDependency' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ghta00001-university-of-saarland/TokenDependency' target=\"_blank\">https://wandb.ai/ghta00001-university-of-saarland/TokenDependency</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ghta00001-university-of-saarland/TokenDependency/runs/1bq4ylg3' target=\"_blank\">https://wandb.ai/ghta00001-university-of-saarland/TokenDependency/runs/1bq4ylg3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   1%|          | 2/392 [01:35<5:10:45, 47.81s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6ac3f336fdd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_initializing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRELATION_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"base_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, experiment_name, save_model, model_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0marc_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arc_scores'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (batch_size, seq_len, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mrel_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rel_scores'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (batch_size, seq_len, seq_len, num_relations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Forward pass through XLM-RoBERTa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mroberta_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlm_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m  \u001b[0;31m# Shape: (batch_size, seq_len, HIDDEN_DIM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    630\u001b[0m                 )\n\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train extended model with adapters\n",
        "extended_model = model_initializing(\"pfeiffer\", hidden_dim=768, output_dim=256, relation_num=RELATION_NUM, trained_base_model=base_model)\n",
        "count_parameters(extended_model)\n",
        "train(extended_model, basque_data, \"Basque_Adapter_Experiment\", save_model=True, model_name=\"pfeiffer_adapter\")"
      ],
      "metadata": {
        "id": "hBdzvnJx-6A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q4FQ9KopQLbj"
      },
      "outputs": [],
      "source": [
        "# Loading dataset from Huggingface\n",
        "dataset = load_dataset(path=DATASET_PATH, name=DATASET_NAME, trust_remote_code=True)\n",
        "\n",
        "# A map from dependency to id (id is literally the index) {key (deprels) : value(indexes)}\n",
        "deprel_to_id = {deprel: idx for idx, deprel in enumerate(ALL_DEPRELS)}\n",
        "\n",
        "# A map from id to dependency (id is literally the index) {value(indexes) : key (deprels)}\n",
        "id_to_deprel = {idx: deprel for idx, deprel in enumerate(ALL_DEPRELS)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxYnIzJWSisL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HfvS14DtTEYE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-McJ-FHGHEx"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g34PomFOzbIy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sly0vkybUJ-x",
        "outputId": "93ab805b-77e1-44fe-e65a-cab7a3f930d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁Al        -> Head: N/A        -> Deprel: root       -> Word: N/A\n",
            "Token : ▁-         -> Head: N/A        -> Deprel: punct      -> Word: -\n",
            "Token : ▁Zaman     -> Head: N/A        -> Deprel: flat       -> Word: Zaman\n",
            "Token : ▁:         -> Head: N/A        -> Deprel: punct      -> Word: :\n",
            "Token : ▁American  -> Head: forces     -> Deprel: amod       -> Word: American\n",
            "Token : ▁forces    -> Head: killed     -> Deprel: nsubj      -> Word: forces\n",
            "Token : ▁killed    -> Head: N/A        -> Deprel: parataxis  -> Word: killed\n",
            "Token : ▁Sha       -> Head: killed     -> Deprel: obj        -> Word: Shaikh\n",
            "Token : ikh        -> Head: N/A        -> Deprel: None       -> Word: Shaikh\n",
            "Token : ▁Abdullah  -> Head: Shaikh     -> Deprel: flat       -> Word: Abdullah\n",
            "Token : ▁al        -> Head: Shaikh     -> Deprel: flat       -> Word: al\n",
            "Token : ▁Ani       -> Head: Shaikh     -> Deprel: punct      -> Word: Ani\n",
            "Token : ▁          -> Head: Shaikh     -> Deprel: flat       -> Word: .\n",
            "Token : ,          -> Head: Shaikh     -> Deprel: punct      -> Word: ,\n",
            "Token : ▁the       -> Head: N/A        -> Deprel: None       -> Word: the\n",
            "Token : ▁prea      -> Head: preacher   -> Deprel: det        -> Word: preacher\n",
            "Token : cher       -> Head: Shaikh     -> Deprel: appos      -> Word: preacher\n",
            "Token : ▁at        -> Head: N/A        -> Deprel: None       -> Word: at\n",
            "Token : ▁mos       -> Head: mosque     -> Deprel: case       -> Word: mosque\n",
            "Token : que        -> Head: mosque     -> Deprel: det        -> Word: mosque\n",
            "Token : ▁in        -> Head: killed     -> Deprel: obl        -> Word: in\n",
            "Token : ▁town      -> Head: N/A        -> Deprel: None       -> Word: town\n",
            "Token : ▁of        -> Head: town       -> Deprel: case       -> Word: of\n",
            "Token : ▁Qa        -> Head: town       -> Deprel: det        -> Word: Qaim\n",
            "Token : im         -> Head: mosque     -> Deprel: nmod       -> Word: Qaim\n",
            "Token : ▁near      -> Head: Qaim       -> Deprel: case       -> Word: near\n",
            "Token : ▁Syria     -> Head: town       -> Deprel: nmod       -> Word: Syrian\n",
            "Token : n          -> Head: N/A        -> Deprel: None       -> Word: Syrian\n",
            "Token : ▁border    -> Head: town       -> Deprel: punct      -> Word: border\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : </s>       -> Head: border     -> Deprel: case       -> Word: N/A\n",
            "Token : <pad>      -> Head: border     -> Deprel: det        -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁[         -> Head: causing    -> Deprel: punct      -> Word: N/A\n",
            "Token : ▁This      -> Head: killing    -> Deprel: det        -> Word: This\n",
            "Token : ▁ki        -> Head: causing    -> Deprel: nsubj      -> Word: killing\n",
            "Token : lling      -> Head: N/A        -> Deprel: None       -> Word: killing\n",
            "Token : ▁of        -> Head: cleric     -> Deprel: case       -> Word: of\n",
            "Token : ▁a         -> Head: cleric     -> Deprel: det        -> Word: a\n",
            "Token : ▁respect   -> Head: cleric     -> Deprel: amod       -> Word: respected\n",
            "Token : ed         -> Head: N/A        -> Deprel: None       -> Word: respected\n",
            "Token : ▁cleric    -> Head: killing    -> Deprel: nmod       -> Word: cleric\n",
            "Token : ▁will      -> Head: causing    -> Deprel: aux        -> Word: will\n",
            "Token : ▁be        -> Head: causing    -> Deprel: aux        -> Word: be\n",
            "Token : ▁causing   -> Head: N/A        -> Deprel: root       -> Word: causing\n",
            "Token : ▁us        -> Head: causing    -> Deprel: iobj       -> Word: us\n",
            "Token : ▁trouble   -> Head: causing    -> Deprel: obj        -> Word: trouble\n",
            "Token : ▁for       -> Head: years      -> Deprel: case       -> Word: for\n",
            "Token : ▁years     -> Head: causing    -> Deprel: obl        -> Word: years\n",
            "Token : ▁to        -> Head: come       -> Deprel: mark       -> Word: to\n",
            "Token : ▁come      -> Head: years      -> Deprel: acl        -> Word: come\n",
            "Token : ▁          -> Head: causing    -> Deprel: punct      -> Word: .\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : ▁]         -> Head: causing    -> Deprel: punct      -> Word: ]\n",
            "Token : </s>       -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : <pad>      -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁D         -> Head: N/A        -> Deprel: root       -> Word: N/A\n",
            "Token : PA         -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁:         -> Head: N/A        -> Deprel: punct      -> Word: :\n",
            "Token : ▁Iraq      -> Head: authorities -> Deprel: amod       -> Word: Iraqi\n",
            "Token : i          -> Head: N/A        -> Deprel: None       -> Word: Iraqi\n",
            "Token : ▁authorities -> Head: announced  -> Deprel: nsubj      -> Word: authorities\n",
            "Token : ▁announced -> Head: N/A        -> Deprel: parataxis  -> Word: announced\n",
            "Token : ▁that      -> Head: busted     -> Deprel: mark       -> Word: that\n",
            "Token : ▁they      -> Head: busted     -> Deprel: nsubj      -> Word: they\n",
            "Token : ▁had       -> Head: busted     -> Deprel: aux        -> Word: had\n",
            "Token : ▁bu        -> Head: announced  -> Deprel: ccomp      -> Word: busted\n",
            "Token : sted       -> Head: N/A        -> Deprel: None       -> Word: busted\n",
            "Token : ▁up        -> Head: busted     -> Deprel: compound:prt -> Word: up\n",
            "Token : ▁3         -> Head: cells      -> Deprel: nummod     -> Word: 3\n",
            "Token : ▁terrorist -> Head: cells      -> Deprel: amod       -> Word: terrorist\n",
            "Token : ▁cell      -> Head: busted     -> Deprel: obj        -> Word: cells\n",
            "Token : s          -> Head: N/A        -> Deprel: None       -> Word: cells\n",
            "Token : ▁operating -> Head: cells      -> Deprel: acl        -> Word: operating\n",
            "Token : ▁in        -> Head: Baghdad    -> Deprel: case       -> Word: in\n",
            "Token : ▁Bagh      -> Head: operating  -> Deprel: obl        -> Word: Baghdad\n",
            "Token : dad        -> Head: N/A        -> Deprel: None       -> Word: Baghdad\n",
            "Token : ▁          -> Head: N/A        -> Deprel: punct      -> Word: .\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : </s>       -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : <pad>      -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁Two       -> Head: run        -> Deprel: nsubj:pass -> Word: N/A\n",
            "Token : ▁of        -> Head: them       -> Deprel: case       -> Word: of\n",
            "Token : ▁them      -> Head: N/A        -> Deprel: nmod       -> Word: them\n",
            "Token : ▁were      -> Head: run        -> Deprel: aux        -> Word: were\n",
            "Token : ▁being     -> Head: run        -> Deprel: aux:pass   -> Word: being\n",
            "Token : ▁run       -> Head: N/A        -> Deprel: root       -> Word: run\n",
            "Token : ▁by        -> Head: officials  -> Deprel: case       -> Word: by\n",
            "Token : ▁2         -> Head: officials  -> Deprel: nummod     -> Word: 2\n",
            "Token : ▁official  -> Head: run        -> Deprel: obl        -> Word: officials\n",
            "Token : s          -> Head: N/A        -> Deprel: None       -> Word: officials\n",
            "Token : ▁the       -> Head: Ministry   -> Deprel: case       -> Word: the\n",
            "Token : ▁Ministry  -> Head: Ministry   -> Deprel: det        -> Word: Ministry\n",
            "Token : ▁Interior  -> Head: officials  -> Deprel: nmod       -> Word: Interior\n",
            "Token : ▁!         -> Head: Interior   -> Deprel: case       -> Word: !\n",
            "Token : </s>       -> Head: Interior   -> Deprel: det        -> Word: N/A\n",
            "Token : <pad>      -> Head: Ministry   -> Deprel: nmod       -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁The       -> Head: MoI        -> Deprel: det        -> Word: N/A\n",
            "Token : ▁Mo        -> Head: equivalent -> Deprel: nsubj      -> Word: MoI\n",
            "Token : I          -> Head: N/A        -> Deprel: None       -> Word: MoI\n",
            "Token : ▁in        -> Head: Iraq       -> Deprel: case       -> Word: in\n",
            "Token : ▁Iraq      -> Head: MoI        -> Deprel: nmod       -> Word: Iraq\n",
            "Token : ▁is        -> Head: equivalent -> Deprel: cop        -> Word: is\n",
            "Token : ▁equivalent -> Head: N/A        -> Deprel: root       -> Word: equivalent\n",
            "Token : ▁to        -> Head: FBI        -> Deprel: case       -> Word: to\n",
            "Token : ▁the       -> Head: FBI        -> Deprel: det        -> Word: the\n",
            "Token : ▁US        -> Head: FBI        -> Deprel: compound   -> Word: US\n",
            "Token : ▁FBI       -> Head: equivalent -> Deprel: obl        -> Word: FBI\n",
            "Token : ▁          -> Head: equivalent -> Deprel: punct      -> Word: .\n",
            "Token : ,          -> Head: N/A        -> Deprel: None       -> Word: ,\n",
            "Token : ▁so        -> Head: having     -> Deprel: advmod     -> Word: so\n",
            "Token : ▁this      -> Head: be         -> Deprel: nsubj      -> Word: this\n",
            "Token : ▁would     -> Head: be         -> Deprel: aux        -> Word: would\n",
            "Token : ▁be        -> Head: equivalent -> Deprel: parataxis  -> Word: be\n",
            "Token : ▁like      -> Head: having     -> Deprel: mark       -> Word: like\n",
            "Token : ▁having    -> Head: be         -> Deprel: advcl      -> Word: having\n",
            "Token : ▁J         -> Head: employ     -> Deprel: nsubj      -> Word: J.\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : ▁Edgar     -> Head: J.         -> Deprel: flat       -> Word: Edgar\n",
            "Token : ▁Ho        -> Head: J.         -> Deprel: flat       -> Word: Hoover\n",
            "Token : over       -> Head: N/A        -> Deprel: None       -> Word: Hoover\n",
            "Token : ▁un        -> Head: employ     -> Deprel: advmod     -> Word: unwittingly\n",
            "Token : wit        -> Head: N/A        -> Deprel: None       -> Word: unwittingly\n",
            "Token : ting       -> Head: N/A        -> Deprel: None       -> Word: unwittingly\n",
            "Token : ly         -> Head: N/A        -> Deprel: None       -> Word: unwittingly\n",
            "Token : ▁employ    -> Head: having     -> Deprel: ccomp      -> Word: employ\n",
            "Token : ▁at        -> Head: level      -> Deprel: case       -> Word: at\n",
            "Token : ▁a         -> Head: level      -> Deprel: det        -> Word: a\n",
            "Token : ▁high      -> Head: level      -> Deprel: amod       -> Word: high\n",
            "Token : ▁level     -> Head: employ     -> Deprel: obl        -> Word: level\n",
            "Token : ▁members   -> Head: employ     -> Deprel: obj        -> Word: members\n",
            "Token : ▁of        -> Head: bombers    -> Deprel: case       -> Word: of\n",
            "Token : ▁Weather   -> Head: bombers    -> Deprel: det        -> Word: Weathermen\n",
            "Token : men        -> Head: bombers    -> Deprel: compound   -> Word: Weathermen\n",
            "Token : ▁bomb      -> Head: N/A        -> Deprel: None       -> Word: bombers\n",
            "Token : ers        -> Head: members    -> Deprel: nmod       -> Word: bombers\n",
            "Token : ▁back      -> Head: N/A        -> Deprel: None       -> Word: back\n",
            "Token : ▁1960      -> Head: 1960s      -> Deprel: advmod     -> Word: 1960s\n",
            "Token : s          -> Head: 1960s      -> Deprel: case       -> Word: 1960s\n",
            "Token : </s>       -> Head: 1960s      -> Deprel: det        -> Word: N/A\n",
            "Token : <pad>      -> Head: employ     -> Deprel: obl        -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁The       -> Head: third      -> Deprel: det        -> Word: N/A\n",
            "Token : ▁third     -> Head: run        -> Deprel: nsubj:pass -> Word: third\n",
            "Token : ▁was       -> Head: run        -> Deprel: aux        -> Word: was\n",
            "Token : ▁being     -> Head: run        -> Deprel: aux:pass   -> Word: being\n",
            "Token : ▁run       -> Head: N/A        -> Deprel: root       -> Word: run\n",
            "Token : ▁by        -> Head: head       -> Deprel: case       -> Word: by\n",
            "Token : ▁the       -> Head: head       -> Deprel: det        -> Word: the\n",
            "Token : ▁head      -> Head: run        -> Deprel: obl        -> Word: head\n",
            "Token : ▁of        -> Head: firm       -> Deprel: case       -> Word: of\n",
            "Token : ▁an        -> Head: firm       -> Deprel: det        -> Word: an\n",
            "Token : ▁investment -> Head: firm       -> Deprel: compound   -> Word: investment\n",
            "Token : ▁firm      -> Head: head       -> Deprel: nmod       -> Word: firm\n",
            "Token : ▁          -> Head: run        -> Deprel: punct      -> Word: .\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : </s>       -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : <pad>      -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "----------------------------------------\n",
            "Token : <s>        -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : ▁You       -> Head: wonder     -> Deprel: nsubj      -> Word: N/A\n",
            "Token : ▁wonder    -> Head: N/A        -> Deprel: root       -> Word: wonder\n",
            "Token : ▁if        -> Head: manipulating -> Deprel: mark       -> Word: if\n",
            "Token : ▁he        -> Head: manipulating -> Deprel: nsubj      -> Word: he\n",
            "Token : ▁was       -> Head: manipulating -> Deprel: aux        -> Word: was\n",
            "Token : ▁manipula  -> Head: wonder     -> Deprel: ccomp      -> Word: manipulating\n",
            "Token : ting       -> Head: N/A        -> Deprel: None       -> Word: manipulating\n",
            "Token : ▁the       -> Head: market     -> Deprel: det        -> Word: the\n",
            "Token : ▁market    -> Head: manipulating -> Deprel: obj        -> Word: market\n",
            "Token : ▁with      -> Head: targets    -> Deprel: case       -> Word: with\n",
            "Token : ▁his       -> Head: targets    -> Deprel: nmod:poss  -> Word: his\n",
            "Token : ▁bomb      -> Head: targets    -> Deprel: compound   -> Word: bombing\n",
            "Token : ing        -> Head: N/A        -> Deprel: None       -> Word: bombing\n",
            "Token : ▁target    -> Head: manipulating -> Deprel: obl        -> Word: targets\n",
            "Token : s          -> Head: N/A        -> Deprel: None       -> Word: targets\n",
            "Token : ▁          -> Head: wonder     -> Deprel: punct      -> Word: .\n",
            "Token : .          -> Head: N/A        -> Deprel: None       -> Word: .\n",
            "Token : </s>       -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "Token : <pad>      -> Head: N/A        -> Deprel: None       -> Word: N/A\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "sample_tokenized_inputs = tokenize_and_align_labels(dataset[\"train\"][:10])\n",
        "explore_some_data(dataset[\"train\"], sample_tokenized_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eRR3d8ySr-Op"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QpzwKXHhwrxX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO6JcsmbdJ4L"
      },
      "source": [
        "Here we initialize the dataset and create the dataloaders, then print the first batch of trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IcZhhIw3u36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3b_1eIZMvMW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dc0HfDGD_Qxm"
      },
      "outputs": [],
      "source": [
        "trained_model = InitialModel(HIDDEN_DIM, OUTPUT_DIM, RELATION_NUM)\n",
        "trained_model.load_state_dict(torch.load(\"model_epoch_3.pth\"))\n",
        "extended_model = ExtendedModelWithHoulsby(trained_model, adapter_dim=64)\n",
        "extended_model.to(device)\n",
        "# Step 3: Train on low-resource language\n",
        "# (Optimizer should only update parameters with requires_grad=True, i.e., adapters)\n",
        "optimizer = torch.optim.Adam(\n",
        "    [p for p in extended_model.parameters() if p.requires_grad],\n",
        "    lr=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SjUg3eWUa9W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3dotLNzU9S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuSh4GJUaslm"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jaUoqyC8OhH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S52HUFu7jxS0",
        "outputId": "17902757-c9f4-4375-b00d-a3e123cd9696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test UAS: 0.8408, LAS: 0.7992\n",
            "Unlabeled Attachment Score (UAS): 0.7942\n"
          ]
        }
      ],
      "source": [
        "# Testing model\n",
        "test_loader = data[\"test\"]\n",
        "test_acc = test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOM1JBMctwu-"
      },
      "source": [
        "TEST : 91.75 percent\n",
        "\n",
        "UAS : percent\n",
        "\n",
        "\n",
        "VALIDATION :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct8taXRziMw-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4innCpsOf4Ka"
      },
      "source": [
        "# NOTE\n",
        "Comments with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkoFC4dNCyt1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FphnL52E_0N"
      },
      "source": [
        "# BIG NOTE\n",
        "\n",
        "It's like 10.10 and ***** Colab stopped :(((((\n",
        "I'll send loss and accuracy figures in email.\n",
        "but you can see results here.\n",
        "Oh DAMNNNNNNNNNNNNNNNNNNNNNNNNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMzOsBNtFO0I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}